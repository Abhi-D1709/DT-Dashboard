import streamlit as st
import pandas as pd
import requests
import numpy as np
import re
import time
import io
import random
from datetime import datetime, timedelta, date
from typing import Dict, Any, Tuple, List, Optional
import concurrent.futures

# -------------------------------
# Page Config
# -------------------------------
st.set_page_config(page_title="DT Compliance Checker", layout="wide")

# -------------------------------
# HTTP / Headers
# -------------------------------
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "en-US,en;q=0.9",
}

BSE_HEADERS = {
    **HEADERS,
    "Origin": "https://www.bseindia.com",
    "Referer": "https://www.bseindia.com/",
}

# -------------------------------
# Period End Dates
# -------------------------------
QUARTER_END_DATES = {"Q1": "06-30", "Q2": "09-30", "Q3": "12-31", "Q4": "03-31"}
HALF_YEAR_END_DATES = {"H1": "09-30", "H2": "03-31"}

# -------------------------------
# Regulatory Checks Dictionary (15 checks)
# -------------------------------
CHECKS_DICT = {
    "Security Cover Certificate": "Quarterly (Q1-Q3: 75 days, Q4: 90 days)",
    "Statement of value of pledged securities": "Quarterly",
    "Statement of value for Debt Service Reserve Account": "Quarterly",
    "Net worth certificate of Personal Guarantor": "Half-Yearly (Within 75 days)",
    "Financials/Value of Corporate Guarantor": "Annual (Within 75 days from FY end)",
    "Valuation Report": "Once every 3 years (Within 75 days from FY end)",
    "Title Search Report": "Once every 3 years (Within 75 days from FY end)",
    "Status Report on breach of covenants": "Quarterly (Within 90 days)",
    "NOC/No dues certificate/consent/permission": "Event Based (Within 2 working days)",
    "Breach of Minimum Security Cover": "Event Based (Within 2 working days)",
    "Default in payment of interest or redemption amount": "Event Based (Within 1 day)",
    "Failure to create charge on assets": "Event Based (Within 1 day)",
    "Disclosure of status of payment of debt securities ‚Äì Defaulted": "Event Based (Within 9 days of maturity)",
    "Status of payment of defaulted debt securities": "Annual (7th working day of April)",
    "Developments that impact the status of default": "Event Based (Within 1 working day)",
}

# -------------------------------
# Helpers: safer HTTP with retries
# -------------------------------
def request_json_with_retries(
    session: requests.Session,
    url: str,
    headers: Optional[dict] = None,
    timeout: int = 15,
    max_tries: int = 4,
    base_sleep: float = 0.8,
) -> Tuple[Optional[Any], Optional[str]]:
    last_err = None
    for attempt in range(1, max_tries + 1):
        try:
            resp = session.get(url, headers=headers, timeout=timeout)
            if resp.status_code == 200:
                try:
                    return resp.json(), None
                except Exception as e:
                    last_err = f"JSON decode error: {e}"
            else:
                last_err = f"HTTP {resp.status_code}"
        except Exception as e:
            last_err = f"Request error: {e}"

        sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0.1, 0.6)
        time.sleep(sleep_s)

    return None, f"Failed after retries: {last_err} | URL: {url}"


def request_text_with_retries(
    session: requests.Session,
    url: str,
    headers: Optional[dict] = None,
    timeout: int = 15,
    max_tries: int = 4,
    base_sleep: float = 0.8,
) -> Tuple[Optional[str], Optional[str]]:
    last_err = None
    for attempt in range(1, max_tries + 1):
        try:
            resp = session.get(url, headers=headers, timeout=timeout)
            if resp.status_code == 200:
                return resp.text, None
            last_err = f"HTTP {resp.status_code}"
        except Exception as e:
            last_err = f"Request error: {e}"

        sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(0.1, 0.6)
        time.sleep(sleep_s)

    return None, f"Failed after retries: {last_err} | URL: {url}"


# -------------------------------
# NSE session bootstrap
# -------------------------------
def make_nse_session() -> requests.Session:
    s = requests.Session()
    s.headers.update(HEADERS)
    try:
        s.get("https://www.nseindia.com", timeout=10)
    except Exception:
        pass
    return s


# -------------------------------
# Background data: Master ISIN list
# -------------------------------
@st.cache_data(ttl=86400)
def get_master_isin_list() -> Dict[str, Dict[str, Any]]:
    """
    Returns dict:
      isin -> {ISIN, BSE_Scrip, NSE_Symbol, Name}
    """
    isin_dict: Dict[str, Dict[str, Any]] = {}
    s = requests.Session()

    # 1) BSE Debentures & Bonds master
    bse_url = (
        "https://api.bseindia.com/BseIndiaAPI/api/ListofScripData/w"
        "?Group=&Scripcode=&segment=Debentures+and+Bonds&status=Active"
    )
    bse_json, bse_err = request_json_with_retries(s, bse_url, headers=BSE_HEADERS, timeout=20)
    if bse_json is None:
        st.warning(f"Could not load BSE master list: {bse_err}")
    else:
        for item in bse_json:
            isin = str(item.get("ISIN_NUMBER", "")).strip()
            if not isin:
                continue
            isin_dict[isin] = {
                "ISIN": isin,
                "BSE_Scrip": str(item.get("SCRIP_CD", "")).strip() or None,
                "Name": str(item.get("Scrip_Name", "")).strip() or "Unknown",
                "NSE_Symbol": None,
            }

    # 2) NSE DEBT.csv master
    nse_url = "https://nsearchives.nseindia.com/content/equities/DEBT.csv"
    csv_text, nse_err = request_text_with_retries(s, nse_url, headers=HEADERS, timeout=20)
    if csv_text is None:
        st.warning(f"Could not load NSE master list: {nse_err}")
    else:
        nse_cols = [
            "SYMBOL", "NAME OF COMPANY", "SERIES", "FACE VALUE", "PAID UP VALUE", "MKT LOT", "IP RATE",
            "DATE OF LISTING", "DATE OF ALLOTMENT", "REDEMPTION DATE", "REDEMPTION AMT",
            "CONVERSION DATE", "CONVERSION AMT", "INTEREST PAYMENT DT", "ISIN"
        ]
        nse_df = pd.read_csv(io.StringIO(csv_text), names=nse_cols, header=0)
        for _, row in nse_df.iterrows():
            isin = str(row.get("ISIN", "")).strip()
            if not isin or isin.upper() == "NAN":
                continue
            comp_name = str(row.get("NAME OF COMPANY", "")).strip()
            symbol = str(row.get("SYMBOL", "")).strip()
            name_to_use = comp_name if comp_name and comp_name.lower() != "nan" else (symbol or "Unknown")

            if isin in isin_dict:
                isin_dict[isin]["NSE_Symbol"] = symbol or None
                if not isin_dict[isin].get("Name") or isin_dict[isin]["Name"] == "Unknown":
                    isin_dict[isin]["Name"] = name_to_use
            else:
                isin_dict[isin] = {
                    "ISIN": isin,
                    "BSE_Scrip": None,
                    "NSE_Symbol": symbol or None,
                    "Name": name_to_use,
                }

    return isin_dict


# -------------------------------
# NSDL DT name fetch
# -------------------------------
def get_dt_name_from_nsdl(isin: str) -> str:
    url = f"https://indiabondinfo.nsdl.com/bds-service/v1/public/bdsinfo/keycontacts?isin={isin}"
    s = requests.Session()
    json_data, err = request_json_with_retries(s, url, headers=HEADERS, timeout=12, max_tries=3)
    
    if json_data:
        dt_name = json_data.get("debtTrusteeName", "")
        dt_name = " ".join(str(dt_name).split())
        return dt_name if dt_name else "Not Available"
    
    return "Not Available"


# -------------------------------
# Classification / parsing
# -------------------------------
def classify_disclosure(text: str) -> str:
    text_lower = str(text).lower()
    mapping = {
        "security cover": "Security Cover Certificate",
        "pledged": "Statement of value of pledged securities",
        "reserve account": "Statement of value for Debt Service Reserve Account",
        "dsra": "Statement of value for Debt Service Reserve Account",
        "net worth": "Net worth certificate of Personal Guarantor",
        "corporate guarantor": "Financials/Value of Corporate Guarantor",
        "valuation": "Valuation Report",
        "title search": "Title Search Report",
        "breach of covenants": "Status Report on breach of covenants",
        "noc": "NOC/No dues certificate/consent/permission",
        "no dues": "NOC/No dues certificate/consent/permission",
        "consent": "NOC/No dues certificate/consent/permission",
        "breach of minimum": "Breach of Minimum Security Cover",
        "default in payment": "Default in payment of interest or redemption amount",
        "failure to create charge": "Failure to create charge on assets",
        "defaulted debt securities": "Disclosure of status of payment of debt securities ‚Äì Defaulted",
        "status of payment of defaulted": "Status of payment of defaulted debt securities",
        "developments that impact": "Developments that impact the status of default",
    }
    for key, standard_name in mapping.items():
        if key in text_lower:
            return standard_name
    return "Other / Miscellaneous"


def extract_nse_period_fy(text: str, default_year: int) -> Tuple[str, int]:
    text_upper = str(text).upper()
    period = "Unknown"
    fy_start = default_year

    fy_match = re.search(r"FY\s*(\d{4})", text_upper) or re.search(r"(20\d{2})-(20)?\d{2}", text_upper)
    if fy_match:
        fy_start = int(fy_match.group(1))

    if re.search(r"\bQ1\b|QUARTER 1|FIRST QUARTER|JUN", text_upper):
        period = "Q1"
    elif re.search(r"\bQ2\b|QUARTER 2|SECOND QUARTER|SEP", text_upper):
        period = "Q2"
    elif re.search(r"\bQ3\b|QUARTER 3|THIRD QUARTER|DEC", text_upper):
        period = "Q3"
    elif re.search(r"\bQ4\b|QUARTER 4|FOURTH QUARTER|MAR", text_upper):
        period = "Q4"
    elif re.search(r"\bH1\b|HALF YEARLY|FIRST HALF", text_upper):
        period = "H1"
    elif re.search(r"\bH2\b|SECOND HALF", text_upper):
        period = "H2"
    elif re.search(r"ANNUAL|YEARLY", text_upper):
        period = "Annual"

    return period, fy_start


def get_due_date(period: str, fin_year_start: int, check_category: str) -> str:
    if "Event Based" in CHECKS_DICT.get(check_category, ""):
        return "Manual Review"

    try:
        if check_category == "Status of payment of defaulted debt securities":
            return str(np.busday_offset(f"{fin_year_start + 1}-04-01", 6, roll="forward"))

        if check_category == "Status Report on breach of covenants" and period in QUARTER_END_DATES:
            year = fin_year_start if period in ["Q1", "Q2", "Q3"] else fin_year_start + 1
            q_end = datetime.strptime(f"{year}-{QUARTER_END_DATES[period]}", "%Y-%m-%d")
            return (q_end + timedelta(days=90)).strftime("%Y-%m-%d")

        if period in QUARTER_END_DATES:
            year = fin_year_start if period in ["Q1", "Q2", "Q3"] else fin_year_start + 1
            q_end = datetime.strptime(f"{year}-{QUARTER_END_DATES[period]}", "%Y-%m-%d")
            return (q_end + timedelta(days=90 if period == "Q4" else 75)).strftime("%Y-%m-%d")

        if period in ["H1", "H2"]:
            h_end = f"{fin_year_start}-{HALF_YEAR_END_DATES['H1']}" if period == "H1" else f"{fin_year_start + 1}-{HALF_YEAR_END_DATES['H2']}"
            return (datetime.strptime(h_end, "%Y-%m-%d") + timedelta(days=75)).strftime("%Y-%m-%d")

        if check_category in ["Financials/Value of Corporate Guarantor", "Valuation Report", "Title Search Report"]:
            return (datetime.strptime(f"{fin_year_start + 1}-03-31", "%Y-%m-%d") + timedelta(days=75)).strftime("%Y-%m-%d")

        return "Manual Review"
    except Exception:
        return "Manual Review"

def get_date_chunks(start_date: date, end_date: date, chunk_days: int = 90) -> List[Tuple[date, date]]:
    """Breaks a large date range into API-safe chunks."""
    chunks = []
    current_start = start_date
    while current_start <= end_date:
        current_end = current_start + timedelta(days=chunk_days - 1)
        if current_end > end_date: current_end = end_date
        chunks.append((current_start, current_end))
        current_start = current_end + timedelta(days=1)
    return chunks

# -------------------------------
# Main fetch: Pure 1-to-1 Fetcher
# -------------------------------
def fetch_disclosures_for_isin(isin_data: Dict[str, Any], from_dt: date, to_dt: date, dt_name_global: str) -> Tuple[pd.DataFrame, List[str]]:
    errors, records = [], []
    
    # 1. BSE Fetch
    bse_scrip = isin_data.get("BSE_Scrip")
    if bse_scrip:
        bse_from, bse_to = from_dt.strftime("%Y%m%d"), to_dt.strftime("%Y%m%d")
        bse_url = f"https://api.bseindia.com/BseIndiaAPI/api/DisclosureDT/w?fromdt={bse_from}&scripcode={bse_scrip}&todt={bse_to}"
        s = requests.Session()
        bse_json, bse_err = request_json_with_retries(s, bse_url, headers=BSE_HEADERS, timeout=15)
        if bse_json is None: errors.append(f"[BSE {bse_from}-{bse_to}] {bse_err}")
        else:
            for item in bse_json.get("Table", []):
                try: sub_date = datetime.strptime(str(item.get("InsDttm", ""))[:10], "%Y-%m-%d")
                except Exception: continue
                fy_raw = item.get("Financial_year")
                fy_start = int(str(fy_raw)[:4]) if fy_raw else sub_date.year
                period, raw_text, doc_link = str(item.get("period", "")).strip(), str(item.get("Body_text", "")).strip(), str(item.get("pdf_name", "")).strip()
                category = classify_disclosure(raw_text)
                due_date_str = get_due_date(period, fy_start, category)
                status = "Manual Review Required" if due_date_str == "Manual Review" else ("On Time" if sub_date <= datetime.strptime(due_date_str, "%Y-%m-%d") else "Delayed")

                records.append({
                    "ISIN": isin_data.get("ISIN"), "Entity": isin_data.get("Name", "Unknown"), "Exchange": "BSE", "Debenture Trustee": dt_name_global,
                    "Check Category": category, "Raw Disclosure Name": raw_text, "Period/FY": f"{period} {fy_start}",
                    "Submission Date": sub_date.strftime("%Y-%m-%d"), "Calculated Due Date": due_date_str, "Compliance Status": status, "Document Link": doc_link,
                })

    # 2. NSE Fetch
    nse_session = make_nse_session()
    nse_from, nse_to = from_dt.strftime("%d-%m-%Y"), to_dt.strftime("%d-%m-%Y")
    nse_url = f"https://www.nseindia.com/api/NextApi/apiClient/GetQuoteApi?functionName=getDTDisclosures&comp_name=&dt_name=&ISIN={isin_data.get('ISIN','')}&subject=&from_date={nse_from}&to_date={nse_to}"

    nse_json, nse_err = request_json_with_retries(nse_session, nse_url, headers=HEADERS, timeout=15)
    if nse_json is None: errors.append(f"[NSE {nse_from}-{nse_to}] {nse_err}")
    else:
        for item in nse_json:
            try: sub_date = datetime.strptime(item.get("exchdissTime", ""), "%d-%b-%Y %H:%M:%S")
            except Exception:
                try: sub_date = datetime.strptime(item.get("dt", ""), "%d-%b-%Y %H:%M:%S")
                except Exception: continue

            raw_text, broad_text, doc_link = str(item.get("desc", "")).strip(), str(item.get("broadText", "")).strip(), str(item.get("attchmntName", "")).strip()
            period, fy_start = extract_nse_period_fy(f"{raw_text} {broad_text}", sub_date.year)
            category = classify_disclosure(raw_text)
            due_date_str = get_due_date(period, fy_start, category)
            status = "Manual Review Required" if due_date_str == "Manual Review" else ("On Time" if sub_date <= datetime.strptime(due_date_str, "%Y-%m-%d") else "Delayed")

            records.append({
                "ISIN": isin_data.get("ISIN"), "Entity": isin_data.get("Name", "Unknown"), "Exchange": "NSE", "Debenture Trustee": dt_name_global,
                "Check Category": category, "Raw Disclosure Name": raw_text, "Period/FY": f"{period} {fy_start}" if period != "Unknown" else "Check Text",
                "Submission Date": sub_date.strftime("%Y-%m-%d"), "Calculated Due Date": due_date_str, "Compliance Status": status, "Document Link": doc_link,
            })

    return pd.DataFrame(records), errors

# -------------------------------
# Checklist builder (per ISIN)
# -------------------------------
def build_checklist_for_isin(isin: str, isin_data: Dict[str, Any], df_disc: pd.DataFrame) -> pd.DataFrame:
    checklist_rows = []
    for check, rule in CHECKS_DICT.items():
        found = (
            df_disc[df_disc["Check Category"] == check].sort_values(by="Submission Date", ascending=False)
            if not df_disc.empty else pd.DataFrame()
        )

        if not found.empty:
            exchanges = ", ".join(found["Exchange"].dropna().unique())
            bse_dates = found[found["Exchange"] == "BSE"]["Submission Date"].tolist()
            nse_dates = found[found["Exchange"] == "NSE"]["Submission Date"].tolist()

            date_strs = []
            if bse_dates:
                date_strs.append(f"BSE: {bse_dates[0]}")
            if nse_dates:
                date_strs.append(f"NSE: {nse_dates[0]}")

            statuses = found["Compliance Status"].tolist()
            overall_status = (
                "On Time" if "On Time" in statuses
                else ("Manual Review Required" if "Manual Review Required" in statuses else "Delayed")
            )

            checklist_rows.append({
                "ISIN": isin,
                "Entity": isin_data.get("Name", "Unknown"),
                "Regulatory Provision": check,
                "Timeline Rule": rule,
                "Status": "‚úÖ Submitted",
                "Exchange(s)": exchanges,
                "Submission Date(s)": " | ".join(date_strs),
                "Compliance": overall_status,
            })
        else:
            checklist_rows.append({
                "ISIN": isin,
                "Entity": isin_data.get("Name", "Unknown"),
                "Regulatory Provision": check,
                "Timeline Rule": rule,
                "Status": "‚ùå Not Found",
                "Exchange(s)": "-",
                "Submission Date(s)": "-",
                "Compliance": "No Data in Period",
            })

    return pd.DataFrame(checklist_rows)


# -------------------------------
# ISIN cleaning
# -------------------------------
ISIN_RE = re.compile(r"^[A-Z]{2}[A-Z0-9]{9}\d$")

def normalize_isin_list(values: List[str]) -> List[str]:
    cleaned = []
    for v in values:
        x = str(v).strip().upper()
        if not x or x in {"NAN", "NONE"}:
            continue
        cleaned.append(x)

    # preserve order + unique
    seen = set()
    uniq = []
    for x in cleaned:
        if x not in seen:
            seen.add(x)
            uniq.append(x)
    return uniq


def highlight_status(val: str) -> str:
    if val == "‚úÖ Submitted":
        return "background-color: #e6ffe6"
    if val == "‚ùå Not Found":
        return "background-color: #ffe6e6"
    return ""

# -------------------------------
# Session State Initialization
# -------------------------------
if "single_results" not in st.session_state:
    st.session_state.single_results = None
if "bulk_results" not in st.session_state:
    st.session_state.bulk_results = None

# -------------------------------
# UI
# -------------------------------
with st.spinner("Syncing latest ISIN master lists from BSE and NSE..."):
    master_isins = get_master_isin_list()

st.title("üìã DT Submissions to SEs - Dashboard")

st.sidebar.header("Audit Parameters")
mode = st.sidebar.radio("Select Input Mode", ["Single ISIN Search", "Bulk Excel Upload"])
start_date = st.sidebar.date_input("From Date", datetime(2025, 10, 1))
end_date = st.sidebar.date_input("To Date", datetime(2025, 12, 31))

if start_date > end_date:
    st.error("From Date cannot be after To Date.")
    st.stop()

# -------------------------------
# Single ISIN
# -------------------------------
if mode == "Single ISIN Search":
    isin_options = list(master_isins.keys())
    selected_isin = st.sidebar.selectbox(
        "Search ISIN (Type to auto-complete)",
        options=[""] + isin_options,
        format_func=lambda x: f"{x} - {master_isins[x]['Name'][:40]}..." if x else "Type or select an ISIN..."
    )

    if st.sidebar.button("Run 15-Point Compliance Check") and selected_isin:
        isin_data = master_isins[selected_isin]
        with st.spinner("Querying NSDL & Exchanges..."):
            master_dt_name = get_dt_name_from_nsdl(selected_isin)
            disclosures_df, errs = fetch_disclosures_for_isin(isin_data, start_date, end_date, master_dt_name)
            checklist_df = build_checklist_for_isin(selected_isin, isin_data, disclosures_df)
            
            # Save to session state so it survives downloads
            st.session_state.single_results = {
                "isin_data": isin_data,
                "dt_name": master_dt_name,
                "disclosures_df": disclosures_df,
                "checklist_df": checklist_df,
                "errs": errs
            }
            # Clear bulk results to avoid confusion if switching modes
            st.session_state.bulk_results = None 

    # Render UI from Session State
    if st.session_state.single_results and mode == "Single ISIN Search":
        res = st.session_state.single_results
        
        st.info(
            f"**Entity:** {res['isin_data'].get('Name')} | **ISIN:** {res['isin_data'].get('ISIN')} | "
            f"**Debenture Trustee:** {res['dt_name']} | **BSE Scrip:** {res['isin_data'].get('BSE_Scrip', 'N/A')} | "
            f"**NSE Match:** {'Yes' if res['isin_data'].get('NSE_Symbol') else 'No'}"
        )

        st.subheader("üìã 15-Point Regulatory Checklist")
        st.dataframe(res['checklist_df'].style.map(highlight_status, subset=["Status"]), use_container_width=True)

        if not res['disclosures_df'].empty:
            st.subheader("üîç Raw Disclosures Found")
            st.dataframe(
                res['disclosures_df'].drop(columns=["Check Category", "ISIN", "Entity"], errors="ignore"),
                use_container_width=True,
                column_config={"Document Link": st.column_config.LinkColumn("Document Link", display_text="Open PDF")}
            )
            
            csv = res['disclosures_df'].to_csv(index=False).encode('utf-8')
            st.download_button("Download Raw Disclosures CSV", data=csv, file_name=f"{res['isin_data'].get('ISIN')}_Disclosures.csv", mime="text/csv")
        else:
            st.warning("No disclosures found for this ISIN in the selected date range.")

        if res['errs']:
            with st.expander("Debug: fetch errors (NSE/BSE/NSDL)"):
                for e in res['errs']:
                    st.write(e)

# -------------------------------
# Bulk Upload View (Maximum Concurrency)
# -------------------------------
elif mode == "Bulk Excel Upload":
    st.sidebar.markdown("### Batch Processing")
    st.sidebar.caption("Warning: Extreme multithreading enabled. IP bans possible.")
    uploaded_file = st.sidebar.file_uploader("Upload Target List", type=["xlsx", "xls", "csv"])

    if uploaded_file and st.sidebar.button("Execute Bulk Audit", type="primary"):
        try:
            if uploaded_file.name.lower().endswith(".csv"): df_upload = pd.read_csv(uploaded_file)
            else: df_upload = pd.read_excel(uploaded_file)

            raw_isins = df_upload.iloc[:, 0].dropna().astype(str).tolist()
            unique_isins = normalize_isin_list(raw_isins)

            if len(unique_isins) == 0:
                st.sidebar.error("No valid ISINs detected.")
                st.stop()
                
            if len(unique_isins) > 50:
                st.sidebar.warning(f"File truncated. Processing first 50 of {len(unique_isins)} ISINs.")
                unique_isins = unique_isins[:50]

            # 1. Flatten the Workload (ISINs x Date Chunks = Total Tasks)
            all_tasks = []
            isin_dt_map = {}
            
            with st.spinner("Pre-fetching NSDL Trustee data to avoid hammering their API..."):
                for isin in unique_isins:
                    isin_dt_map[isin] = get_dt_name_from_nsdl(isin)
                    
            date_chunks = get_date_chunks(start_date, end_date, 90)
            for isin in unique_isins:
                isin_data = master_isins.get(isin, {"ISIN": isin, "Name": "Unknown Entity", "BSE_Scrip": None, "NSE_Symbol": None})
                for c_start, c_end in date_chunks:
                    all_tasks.append((isin, isin_data, isin_dt_map[isin], c_start, c_end))

            st.warning(f"üö® Firing {len(all_tasks)} simultaneous API requests across {len(unique_isins)} ISINs...")

            # 2. Fire the Thread Pool
            raw_results = {isin: [] for isin in unique_isins}
            debug_errs = {isin: [] for isin in unique_isins}
            
            progress_bar = st.progress(0.0)
            status_text = st.empty()
            
            def worker(task):
                isin, isin_data, dt_name, c_start, c_end = task
                time.sleep(random.uniform(0.1, 0.5)) # Micro-jitter to prevent immediate WAF triggers
                df, errs = fetch_disclosures_for_isin(isin_data, c_start, c_end, dt_name)
                return isin, df, errs

            completed = 0
            # max_workers=20 means 20 requests hitting BSE/NSE at the exact same time
            with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
                futures = {executor.submit(worker, t): t for t in all_tasks}
                for future in concurrent.futures.as_completed(futures):
                    isin, df, errs = future.result()
                    if not df.empty: raw_results[isin].append(df)
                    if errs: debug_errs[isin].extend(errs)
                        
                    completed += 1
                    progress_bar.progress(completed / len(all_tasks))
                    status_text.text(f"API Blast Progress: {completed}/{len(all_tasks)} chunks processed...")
                    
            progress_bar.empty()
            status_text.empty()
            
            # 3. Stitch Everything Back Together
            all_disclosures, all_checklists, debug_rows = [], [], []
            for isin in unique_isins:
                dfs = raw_results[isin]
                isin_data = master_isins.get(isin, {"ISIN": isin, "Name": "Unknown Entity", "BSE_Scrip": None, "NSE_Symbol": None})
                
                if dfs:
                    combined_df = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=["Exchange", "Raw Disclosure Name", "Submission Date", "Document Link"])
                    all_disclosures.append(combined_df)
                    all_checklists.append(build_checklist_for_isin(isin, isin_data, combined_df))
                else:
                    all_checklists.append(build_checklist_for_isin(isin, isin_data, pd.DataFrame()))
                    
                debug_rows.append({"ISIN": isin, "Entity": isin_data.get("Name", "Unknown"), "Errors": " | ".join(debug_errs[isin]) if debug_errs[isin] else "None", "Found": int(len(combined_df)) if dfs else 0})

            # Save to session state
            st.session_state.bulk_results = {
                "checklists": pd.concat(all_checklists, ignore_index=True) if all_checklists else pd.DataFrame(),
                "disclosures": pd.concat(all_disclosures, ignore_index=True) if all_disclosures else pd.DataFrame(),
                "debug": pd.DataFrame(debug_rows).sort_values(by="Found", ascending=True),
                "count": len(unique_isins)
            }
            st.session_state.single_results = None 
            
        except Exception as e:
            st.error(f"Processing failed: {e}")

    # Render UI from Session State
    if st.session_state.bulk_results and mode == "Bulk Excel Upload":
        res = st.session_state.bulk_results
        
        m1, m2 = st.columns(2)
        m1.metric("Batch Size Processed", res['count'])
        m2.metric("Total Submissions Found", len(res['disclosures']))
        st.markdown("<br>", unsafe_allow_html=True)

        if not res['checklists'].empty:
            st.subheader("üìã Master Gap Analysis Checklist")
            st.dataframe(res['checklists'].style.map(highlight_status, subset=["Status"]), use_container_width=True)
            csv_check = res['checklists'].to_csv(index=False).encode("utf-8")
            st.download_button("üì• Export Gap Analysis (CSV)", data=csv_check, file_name=f"Bulk_Gap_Analysis_{start_date}_to_{end_date}.csv", mime="text/csv", type="primary")
            st.markdown("---")

        if not res['disclosures'].empty:
            st.subheader("üìÅ Aggregated Filings Database")
            st.dataframe(res['disclosures'].drop(columns=["Check Category"], errors="ignore"), use_container_width=True, column_config={"Document Link": st.column_config.LinkColumn("Document Link", display_text="View PDF")})
            csv_raw = res['disclosures'].to_csv(index=False).encode("utf-8")
            st.download_button("üì• Export Filings Database (CSV)", data=csv_raw, file_name=f"Bulk_Filings_{start_date}_to_{end_date}.csv", mime="text/csv", type="secondary")

        with st.expander("System Logs & Errors (Check here for WAF Bans)"):
            st.dataframe(res['debug'], use_container_width=True)